{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.style.use('seaborn-paper')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', category=DeprecationWarning)\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Permute\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "\n",
    "from utils.generic_utils import load_dataset_at, calculate_dataset_metrics, cutoff_choice, \\\n",
    "                                cutoff_sequence\n",
    "from utils.constants import MAX_NB_VARIABLES, MAX_TIMESTEPS_LIST\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape, GRU\n",
    "from keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n",
    "\n",
    "from utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\n",
    "from utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\n",
    "from utils.keras_utils import train_model, evaluate_model, set_trainable\n",
    "from utils.layer_utils import AttentionLSTM\n",
    "\n",
    "import time\n",
    "\n",
    "DATASET_INDEX = 14\n",
    "\n",
    "MAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\n",
    "MAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\n",
    "NB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n",
    "\n",
    "TRAINABLE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set directory path\n",
    "path = \"D:/Users/hjcam/Documents/Python Scripts/ECTE355 Project/\"\n",
    "os.chdir( path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = {\"Normal\":{},\"Aggressive\":{},\"Drowsy\":{}}\n",
    "\n",
    "#input directory with data file for importing\n",
    "\n",
    "for behaviour in os.listdir('Dataset'):\n",
    "    print(behaviour)\n",
    "    for drive in os.listdir('Dataset/'+behaviour):\n",
    "        dataset = []\n",
    "        #load text data\n",
    "        acc = np.loadtxt('Dataset/' + behaviour + '/' + drive + '/RAW_ACCELEROMETERS.txt')\n",
    "        gps = np.loadtxt('Dataset/' + behaviour + '/' + drive + '/RAW_GPS.txt')\n",
    "        lane = np.loadtxt('Dataset/' + behaviour + '/' + drive + '/PROC_LANE_DETECTION.txt')\n",
    "        veh = np.loadtxt('Dataset/' + behaviour + '/' + drive + '/PROC_VEHICLE_DETECTION.txt')\n",
    "        \n",
    "        #select desired features\n",
    "        acc_reduced = acc[:,[0,5,6,7,8,9,10]] #time, X KF,Y KF, Z KF, Roll, Pitch, Yaw\n",
    "        gps_reduced = gps[:,[0,1]] #time, speed\n",
    "        lane_reduced = lane[:,[0,1]] #time, lane center\n",
    "        veh_reduced = veh[:,[0,1,2]] #time, dist. to veh, time to vehicle\n",
    "        \n",
    "        #transform in Pandas dataframe\n",
    "        acc_data = pd.DataFrame(acc_reduced, columns = ['Time','X KF','Y KF', 'Z KF', 'Roll', 'Pitch', 'Yaw'])\n",
    "        acc_data.set_index('Time',inplace = True)\n",
    "        gps_data = pd.DataFrame(gps_reduced, columns = ['Time','Speed'])\n",
    "        gps_data.set_index('Time',inplace = True)\n",
    "        lane_data = pd.DataFrame(lane_reduced, columns = ['Time','Lane_X'])\n",
    "        lane_data.set_index('Time',inplace = True)\n",
    "        veh_data = pd.DataFrame(veh_reduced, columns = ['Time','Veh_Dist','Veh_Time'])\n",
    "        veh_data.set_index('Time',inplace = True)\n",
    "        \n",
    "        acc_data = acc_data.groupby(acc_data.index).first()\n",
    "        gps_data = gps_data.groupby(gps_data.index).first()\n",
    "        lane_data = lane_data.groupby(lane_data.index).first()\n",
    "        veh_data = veh_data.groupby(veh_data.index).first()\n",
    "        \n",
    "        #combine all datasets\n",
    "        combined_data = pd.concat([acc_data, gps_data], axis=1, sort=False)\n",
    "        \n",
    "        #synchronise sampling rates\n",
    "        combined_data['Speed'].interpolate(method = 'index',inplace = True)\n",
    "        #combined_data['Lane_X'].interpolate(method = 'index',inplace = True)\n",
    "        #combined_data['Veh_Dist'] = combined_data['Veh_Dist'].fillna(method='ffill')\n",
    "        #combined_data['Veh_Time'] = combined_data['Veh_Time'].fillna(method='ffill')\n",
    "        combined_data.dropna(inplace = True)\n",
    "        combined_data = combined_data[['X KF','Y KF', 'Z KF', 'Roll', 'Pitch', 'Yaw','Speed']]#],'Lane_X','Veh_Dist','Veh_Time']]\n",
    "        \n",
    "        #output combined imported data to dictionary\n",
    "        data[behaviour].update({drive:combined_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the overall formatted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sliding window dataset split, input data, window size, behaviour and overlap\n",
    "def split_sequences(sequences, n_steps, behaviour, n_overlap):\n",
    "    X , y = list() , list()\n",
    "    for i in np.arange(0,len(sequences),n_overlap,dtype = np.int16):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset stop\n",
    "        if end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x = sequences[i:end_ix, :]\n",
    "        #X.append(list(map(list, zip(*seq_x))))\n",
    "        X.append(seq_x)\n",
    "        #insert class into seperate array\n",
    "        if behaviour == 'Normal':\n",
    "            y.append([0])\n",
    "        elif behaviour == 'Drowsy':\n",
    "            y.append([1])\n",
    "        elif behaviour == 'Aggressive':\n",
    "            y.append([2])\n",
    "            \n",
    "    return X , y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate model, only requires shape of data set\n",
    "def generate_model():\n",
    "    ip = Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "    #LSTM component\n",
    "    x = Masking()(ip)\n",
    "    x = LSTM(16)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    #FCN component\n",
    "    y = Permute((2, 1))(ip)\n",
    "    y = Conv1D(128, 8, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = squeeze_excite_block(y)\n",
    "\n",
    "    y = Conv1D(256, 5, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = squeeze_excite_block(y)\n",
    "\n",
    "    y = Conv1D(128, 3, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    y = GlobalAveragePooling1D()(y)\n",
    "    #combine the two model architectures\n",
    "    x = concatenate([x, y])\n",
    "\n",
    "    out = Dense(3, activation='softmax')(x)\n",
    "\n",
    "    model = Model(ip, out)\n",
    "    #model.summary()\n",
    "\n",
    "    # add load model code here to fine-tune\n",
    "\n",
    "    return model\n",
    "\n",
    "#squeeze excite block taken from literature\n",
    "\n",
    "def squeeze_excite_block(input):\n",
    "    ''' Create a squeeze-excite block\n",
    "    Args:\n",
    "        input: input tensor\n",
    "        filters: number of output filters\n",
    "        k: width factor\n",
    "    Returns: a keras tensor\n",
    "    '''\n",
    "    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n",
    "\n",
    "    se = GlobalAveragePooling1D()(input)\n",
    "    se = Reshape((1, filters))(se)\n",
    "    se = Dense(filters // 16,  activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = multiply([input, se])\n",
    "    return se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train_x = []\n",
    "test_train_y = []\n",
    "\n",
    "results_iter = {}\n",
    "\n",
    "# set hyper parameters\n",
    "\n",
    "steps = 300\n",
    "overlap = 50\n",
    "\n",
    "epochs=300\n",
    "batch_size=128\n",
    "counter = 0;\n",
    "\n",
    "#create dataset through sliding window\n",
    "        \n",
    "for key in data:\n",
    "    for drive in data[key]:\n",
    "        x_windows, y_windows = split_sequences(data[key][drive].values, steps, key, overlap)\n",
    "        test_train_x.extend(x_windows)\n",
    "        test_train_y.extend(y_windows)\n",
    "        \n",
    "y = array(test_train_y)\n",
    "x = array(test_train_x)\n",
    "\n",
    "#normalise data\n",
    "        \n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "for data_x in x:\n",
    "    scaler.partial_fit(data_x)\n",
    "\n",
    "scaled_x = []\n",
    "        \n",
    "# Second pass\n",
    "for data_x in x:\n",
    "    scaled_x.append(list(map(list, zip(*scaler.transform(data_x)))))\n",
    "    \n",
    "#create five folds for training\n",
    "    \n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.3, random_state=22)\n",
    "sss.get_n_splits(array(scaled_x), array(y))\n",
    "\n",
    "#train and test on each of the five folds\n",
    "                 \n",
    "for train_index, test_index in sss.split(array(scaled_x), array(y)):\n",
    "    x_train, x_test = array(scaled_x)[train_index], array(scaled_x)[test_index]\n",
    "    y_train_numbers, y_test_numbers = array(y)[train_index], array(y)[test_index]\n",
    "    \n",
    "    #set more hyper parameters and model checkpoints\n",
    "    \n",
    "    learning_rate= 1e-3\n",
    "    monitor='loss'\n",
    "    optimization_mode='auto'\n",
    "    compile_model=True\n",
    "    factor = 1. / np.cbrt(2)\n",
    "    \n",
    "    #location to save model weights\n",
    "    \n",
    "    weight_fn = \"./weights/%s_weights.h5\" % counter\n",
    "    \n",
    "    #reducing learning rate on a loss plateau\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(monitor=monitor, patience=100, mode=optimization_mode,\n",
    "                                  factor=factor, cooldown=0, min_lr=1e-4, verbose=2)\n",
    "    \n",
    "    #save model weights on checkpoint if the loss is the minimal seen so far in all training epochs\n",
    "    \n",
    "    model_checkpoint = ModelCheckpoint(weight_fn, verbose=0, mode=optimization_mode,\n",
    "                                       monitor=monitor, save_best_only=True, save_weights_only=True)\n",
    "    callback_list = [reduce_lr, model_checkpoint]\n",
    "\n",
    "    optm = Adam(lr=learning_rate)\n",
    "    \n",
    "    #generate model\n",
    "        \n",
    "    model = generate_model()\n",
    "    model.compile(optimizer=optm, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "    classes = np.unique(y_train_numbers)\n",
    "    le = LabelEncoder()\n",
    "    y_ind = le.fit_transform(y_train_numbers.ravel())\n",
    "    recip_freq = len(y_train_numbers) / (len(le.classes_) * np.bincount(y_ind).astype(np.float64))\n",
    "    class_weight = recip_freq[le.transform(classes)]\n",
    "        \n",
    "    y_train = to_categorical(y_train_numbers, len(np.unique(y_train_numbers)))\n",
    "    y_test = to_categorical(y_test_numbers, len(np.unique(y_test_numbers)))\n",
    "    \n",
    "    #train model\n",
    "\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "                class_weight=class_weight, verbose=0, callbacks=callback_list, validation_data=(x_test, y_test))\n",
    "    \n",
    "    #finished training, reload best weights\n",
    "    \n",
    "    model.load_weights(weight_fn)\n",
    "    \n",
    "    #time how long for classification\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #predict on test set\n",
    "    \n",
    "    y_pred = model.predict(x_test, batch_size=64, verbose=1)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print('Total time: %s'%(end_time-start_time))\n",
    "    \n",
    "    #print the classification performance and save to array\n",
    "    \n",
    "    y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "    y_pred = to_categorical(y_pred_bool, len(np.unique(y_pred_bool)))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "        \n",
    "    results_iter[str(steps)+'_'+str(overlap)] = classification_report(y_test, y_pred)\n",
    "    \n",
    "    counter = counter + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
